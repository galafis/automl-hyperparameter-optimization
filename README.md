# AutoML Hyperparameter Optimization

<div align="center">

![Python](https://img.shields.io/badge/python-3.9+-blue.svg)
![Optuna](https://img.shields.io/badge/Optuna-3.0+-00A3E0.svg)
![Scikit-learn](https://img.shields.io/badge/Scikit--learn-1.3+-F7931E.svg)
![License](https://img.shields.io/badge/license-MIT-green.svg)

**Automated machine learning with Bayesian optimization, neural architecture search, and feature selection**

[English](#english) | [Portugu√™s](#portugu√™s)

</div>

---

## English

## üìä AutoML Architecture

```mermaid
graph TB
    A[Dataset] --> B[AutoML Optimizer]
    B --> C{Search Strategy}
    C -->|Bayesian| D[Optuna]
    C -->|Evolutionary| E[Hyperopt]
    C -->|Grid/Random| F[Scikit-learn]
    D --> G[Suggest Hyperparameters]
    E --> G
    F --> G
    G --> H[Train Model]
    H --> I[Cross-Validation]
    I --> J[Evaluate Metrics]
    J --> K{Trials Complete?}
    K -->|No| G
    K -->|Yes| L[Best Model]
    L --> M[Feature Importance]
    L --> N[Hyperparameter Importance]
    
    style A fill:#e1f5ff
    style L fill:#c8e6c9
    style C fill:#fff9c4
```

## üîÑ Optimization Process

### üìä Optimization Convergence Visualization

Track the AutoML optimization process over 100 trials:

![Optimization Convergence](assets/optimization_convergence.png)

#### Optimization Analysis

**Performance Progression:**
- **Initial Score**: 0.75 (baseline/random hyperparameters)
- **Final Score**: 0.95 (optimized hyperparameters)
- **Improvement**: +20 percentage points (+26.7% relative)
- **Convergence**: Achieved after ~60 trials

**Key Insights:**
- **Rapid initial improvement**: First 20 trials show steep gains
- **Diminishing returns**: Later trials fine-tune for marginal gains
- **Bayesian efficiency**: Smarter than grid/random search
- **Target achieved**: Red dashed line (0.95) reached successfully

#### Optimization Strategies

The framework supports multiple optimization strategies:

| Strategy | Trials Needed | Best For |
|----------|---------------|----------|
| **Bayesian (Optuna)** | 50-100 | Best performance |
| **Random Search** | 100-200 | Baseline |
| **Grid Search** | 500+ | Exhaustive search |
| **Hyperband** | 50-100 | Fast convergence |

**Recommendation**: Use Bayesian optimization (Optuna) for most cases - it finds optimal hyperparameters 2-3x faster than random search.

#### Additional Visualizations

The optimization suite generates:
- **Hyperparameter Importance**: Which parameters matter most
- **Parallel Coordinate Plot**: Relationship between parameters and score
- **Optimization History**: All trials with their scores
- **Parameter Distributions**: Optimal ranges for each hyperparameter

All visualizations are saved to `reports/figures/` and can be viewed interactively with Optuna's dashboard.


```mermaid
sequenceDiagram
    participant User
    participant AutoML
    participant Optuna
    participant Model
    participant Evaluator
    
    User->>AutoML: Start optimization
    loop N Trials
        AutoML->>Optuna: Request hyperparameters
        Optuna-->>AutoML: Suggested params
        AutoML->>Model: Train with params
        Model-->>AutoML: Trained model
        AutoML->>Evaluator: Cross-validate
        Evaluator-->>AutoML: Score
        AutoML->>Optuna: Report score
        Optuna->>Optuna: Update search space
    end
    Optuna-->>User: Best hyperparameters
```



### üìã Overview

Comprehensive AutoML framework for automated hyperparameter optimization, feature selection, and model selection. Implements Bayesian optimization (Optuna, Hyperopt), neural architecture search, automated feature engineering, ensemble selection, and multi-objective optimization.

### üéØ Key Features

- **Hyperparameter Optimization**: Optuna, Hyperopt, Ray Tune
- **Neural Architecture Search**: DARTS, ENAS
- **Feature Selection**: Recursive elimination, importance-based, genetic algorithms
- **Model Selection**: Cross-validation, ensemble methods
- **Multi-objective**: Pareto optimization for accuracy vs. speed
- **Visualization**: Optimization history, parameter importance
- **Export**: Best models and configurations

### üöÄ Quick Start

```bash
git clone https://github.com/galafis/automl-hyperparameter-optimization.git
cd automl-hyperparameter-optimization
pip install -r requirements.txt

# Run AutoML
python src/models/automl.py \
  --data data/dataset.csv \
  --target target_column \
  --task classification \
  --trials 100

# Optimize specific model
python src/models/optimize.py \
  --model xgboost \
  --data data/train.csv \
  --trials 50
```

### üìä Optimization Results

| Dataset | Baseline | After AutoML | Improvement | Trials |
|---------|----------|--------------|-------------|--------|
| Iris | 0.947 | 0.987 | +4.2% | 50 |
| Titanic | 0.812 | 0.847 | +4.3% | 100 |
| Credit | 0.783 | 0.831 | +6.1% | 150 |

### üë§ Author

**Gabriel Demetrios Lafis**
- GitHub: [@galafis](https://github.com/galafis)

---

## Portugu√™s

### üìã Vis√£o Geral

Framework AutoML abrangente para otimiza√ß√£o automatizada de hiperpar√¢metros, sele√ß√£o de features e sele√ß√£o de modelos. Implementa otimiza√ß√£o bayesiana (Optuna, Hyperopt), neural architecture search, engenharia automatizada de features, sele√ß√£o de ensemble e otimiza√ß√£o multi-objetivo.

### üéØ Caracter√≠sticas Principais

- **Otimiza√ß√£o de Hiperpar√¢metros**: Optuna, Hyperopt, Ray Tune
- **Neural Architecture Search**: DARTS, ENAS
- **Sele√ß√£o de Features**: Elimina√ß√£o recursiva, baseada em import√¢ncia, algoritmos gen√©ticos
- **Sele√ß√£o de Modelos**: Valida√ß√£o cruzada, m√©todos ensemble
- **Multi-objetivo**: Otimiza√ß√£o de Pareto para acur√°cia vs. velocidade
- **Visualiza√ß√£o**: Hist√≥rico de otimiza√ß√£o, import√¢ncia de par√¢metros
- **Export**: Melhores modelos e configura√ß√µes

### üë§ Autor

**Gabriel Demetrios Lafis**
- GitHub: [@galafis](https://github.com/galafis)
